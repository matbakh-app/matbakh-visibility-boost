# Task 12: Microservices Foundation - Praxisnahe Implementation Guide

**Date**: 2025-09-22  
**Status**: Ready for Implementation  
**Dependencies**: Task 9 (Deployment), Task 10 (Auto-Scaling), Task 11 (Multi-Region)

## üéØ Was-muss-ich-beachten-Liste f√ºr Task 12

Diese praxisnahe Liste f√ºhrt Kiro zielgenau durch Architektur, Sicherheit, Betrieb, Tests und Kosten f√ºr eine enterprise-grade Microservices Foundation.

## 1) üèóÔ∏è Architektur-Entscheidungen (fix setzen, bevor gebaut wird)

### Services zuschneiden & Ownership
- **Klare Dom√§nenschnitte**: auth, persona, vc-start, upload, billing
- **Database per service** (logisch) + Eventing f√ºr Integrationen (EventBridge/SNS/SQS)
- **Ownership-Prinzip**: Jeder Service besitzt seine Daten und APIs vollst√§ndig

### Orchestrierung
- **ECS on Fargate** pro Umgebung & Region (eu-central-1 prim√§r, eu-west-1 sekund√§r)
- **Konsistent mit Task 11**: Gleiche Regionen, gleiche Failover-Strategien
- **Capacity Providers**: FARGATE + FARGATE_SPOT f√ºr Cost-Optimization

### Service Mesh
- **AWS App Mesh** mit Envoy Sidecar je Task
- **mTLS automatisch** via ACM Private CA und SPIFFE-IDs
- **Traffic-Splits** f√ºr Canary (1-5%), Retries/Timeouts/Circuit-Breaking
- **Observability** via X-Ray und CloudWatch Metrics

### In/Out Routing
- **Intern**: Cloud Map (Service Discovery) + App Mesh (VirtualService/VirtualRouter/VirtualNode)
- **Extern**: ALB (public) ‚Üí Ingress-Service ‚Üí App Mesh ‚Üí interne Services
- **Path-based Routing**: `/api/auth`, `/api/persona`, `/api/vc` √ºber ALB

### Protokolle
- **REST (HTTP/1.1)** f√ºr BFF/extern und Legacy-Kompatibilit√§t
- **gRPC (HTTP/2)** intern wenn m√∂glich - reduziert Latenz & Payloads
- **App Mesh unterst√ºtzt** beide Protokolle nahtlos

### Konfig & Secrets
- **SSM Parameter Store** f√ºr Konfiguration (nicht-sensitiv)
- **Secrets Manager** f√ºr Credentials mit automatischer Rotation
- **Niemals Klartext** in Container Images oder Environment Variables

## 2) üîí Netzwerk & Sicherheit (ohne das geht nichts)

### VPC Layout
- **3 AZs** pro Region f√ºr High Availability
- **Private Subnets** f√ºr ECS Tasks (keine √∂ffentlichen IPs)
- **Public Subnets** nur f√ºr ALB und NAT Gateways

### Kostenfalle NAT vermeiden
- **VPC Endpoints** f√ºr ECR (api+dkr), CloudWatch Logs, SSM/Secrets
- **Minimaler NAT-Traffic** durch Endpoint-Nutzung
- **Cost-Monitoring** f√ºr NAT Gateway Data Processing Charges

### Security Groups
- **Least Privilege** pro Service
- **Eingehend**: nur Mesh/ALB/abh√§ngige Services
- **Ausgehend**: nur n√∂tige AWS-Endpoints und Service-Dependencies
- **Port-spezifisch**: 8080 f√ºr App, 15000 f√ºr Envoy Admin, 9901 f√ºr Envoy Stats

### App Mesh mTLS
- **ACM Private CA** f√ºr Certificate Authority
- **SPIFFE-IDs** automatisch via App Mesh/Envoy f√ºr Service-Identit√§t
- **Certificate Rotation** automatisch ohne Service-Downtime
- **Mutual Authentication** zwischen allen Services

### IAM
- **Getrennte Rollen**: taskExecutionRole (Pull/Logs) vs taskRole (AWS-Zugriffe)
- **Permissions Boundaries** zur Begrenzung maximaler Berechtigungen
- **Kein Wildcard `*`** - spezifische Resource ARNs verwenden
- **Cross-Account Access** f√ºr Multi-Region Deployments

## 3) üê≥ Compute & Images

### ECR Replikation
- **Multi-Region Replication** aktivieren (Task 11-Konformit√§t)
- **Lifecycle Policies** f√ºr automatische Image-Bereinigung
- **Vulnerability Scanning** bei jedem Push aktiviert

### Image Build & Scan
- **GitHub Actions** ‚Üí BuildKit ‚Üí Trivy Security Scan ‚Üí ECR push
- **Tags**: sha, env, service f√ºr eindeutige Identifikation
- **Multi-Stage Builds** f√ºr minimale Production Images
- **Distroless Base Images** f√ºr Security

### Fargate Tuning
- **Envoy Overhead**: ~0.25 vCPU/256 MB zus√§tzlich pro Task
- **CPU/Memory Planning** mit Sidecar einrechnen
- **Ephemeral Storage** bei Bedarf >20 GB setzen
- **Task Size Limits** beachten (0.25-4 vCPU, 512MB-30GB)

## 4) üï∏Ô∏è App Mesh ‚Äì konkrete Policies (DoRs)

### VirtualRouter Routen
- **Timeouts**: Client 2-3s, Max 5s f√ºr externe APIs
- **Retry**: 5xx, connect-failure, gateway-error, retriable-status
- **Backoff**: 25-250ms mit Jitter f√ºr Thundering Herd Prevention
- **Max Retries**: 3-5 je nach Service-Kritikalit√§t

### Circuit Breaking/OutlierDetection
- **Max Connections**: 1024 pro Upstream Service
- **Ejection**: 5xx ratio >50% f√ºr 30s Base Ejection
- **Max Ejection**: 300s, Max 50% der Instances
- **Health Check**: Interval 30s, Timeout 5s

### Traffic Splits
- **Canary**: 1-5% √ºber VirtualRouter ‚Üí neue VirtualNode-Version
- **Promotion**: Schrittweise 5% ‚Üí 25% ‚Üí 50% ‚Üí 100%
- **Rollback**: Sofort auf 0% bei Fehlern
- **Integration** mit Health Gates (Task 9/10)

## 5) üîç Service Discovery & Load Balancing

### AWS Cloud Map
- **Namespace**: `svc.local` (intern)
- **DNS Records**: A/AAAA + SRV f√ºr Port-Discovery
- **TTL**: 5-10s (Trade-off: DNS Churn vs. Reaktionszeit)
- **Health Checks**: Custom Config mit Failure Threshold

### ALB Configuration
- **Path-based Routing**: `/api/auth`, `/api/persona`, `/api/vc`
- **Target Groups**: Ein Target Group pro Service
- **Health Checks**: `/health` Endpoint mit 200 OK
- **Integration**: Mit bestehender CloudFront Distribution

### Load Balancing Algorithmus
- **Round Robin** f√ºr gleichm√§√üige Verteilung
- **Least Outstanding Requests** f√ºr Performance-kritische Services
- **Sticky Sessions** nur wenn absolut notwendig (stateless bevorzugt)

## 6) üîÑ Inter-Service Kommunikation (Patterns)

### Sync (Mesh)
- **gRPC/HTTP** mit mTLS f√ºr Service-to-Service
- **Idempotenz-Keys** bei "at-least-once" Calls
- **Timeout Hierarchie**: Client < Service < Infrastructure
- **Request IDs** f√ºr End-to-End Tracing

### Async (Eventing)
- **EventBridge/SNS+SQS** f√ºr lose Kopplung
- **Outbox-Pattern** f√ºr Transaktionssicherheit
- **Sagas** f√ºr verteilte Workflows
- **Dead Letter Queues** f√ºr Failed Message Handling

### Schema-Governance
- **API-Vertr√§ge**: OpenAPI f√ºr REST, protobuf f√ºr gRPC
- **Schema-Registry**: Repository-intern + CI-Checks
- **Backward Compatibility** f√ºr Rolling Updates
- **Version Strategy**: Semantic Versioning f√ºr APIs

## 7) üìä Observability (Pflichtprogramm)

### Logging
- **JSON strukturiert** mit einheitlichem Schema
- **Correlation ID/Trace ID** in jedem Log-Entry
- **Log Levels**: ERROR, WARN, INFO, DEBUG mit Environment-spezifischen Levels
- **Centralized Logging** via CloudWatch Logs mit Retention Policies

### Tracing
- **ADOT Collector** Sidecar/Daemon ‚Üí AWS X-Ray
- **W3C Traceparent** Propagation zwischen Services
- **Sampling Rate**: 10% in Production, 100% in Development
- **Custom Spans** f√ºr Business Logic Tracing

### Metrics
- **RED Metrics**: Requests, Errors, Duration pro Service
- **USE Metrics**: Utilization, Saturation, Errors f√ºr Resources
- **Custom Metrics**: Business-spezifische KPIs
- **Namespace**: `matbakh/microservices/{service}`

### Dashboards & Alarms
- **CloudWatch Dashboards** pro Service und Gesamt-Overview
- **Alarme**: P95+P99 Latency, Error Rate >1%, 5xx Rate
- **Composite Alarms** f√ºr Service Health Status
- **Integration** mit bestehenden Monitoring (Task 1)

## 8) üìà Autoscaling (Abgleich mit Task 10)

### ECS Service Auto Scaling
- **CPU**: 60-70% Target f√ºr normale Services
- **Memory**: 70-80% Target f√ºr Memory-intensive Services
- **Custom Metrics**: SQS QueueDepth ‚Üí Tasks, ALB RequestCount
- **Scaling Policies**: Target Tracking + Step Scaling f√ºr Burst

### Lambda vs ECS Entscheidung
- **Provisioned Concurrency** bleibt f√ºr Latency-kritische Lambda-Routen
- **ECS Services** f√ºr l√§ngerlaufende, stateful Services
- **Hybrid Approach**: Schrittweise Migration ohne Doppelzust√§ndigkeit
- **Cost Comparison** pro Endpoint dokumentieren

### Budgets
- **‚Ç¨100/Monat** Baseline (bis ‚Ç¨200 mit Approval)
- **NAT-Reduktion** durch VPC Endpoints
- **Right-Sizing** basierend auf Monitoring
- **Envoy-Overhead** in Budget-Planung einrechnen

## 9) üöÄ CI/CD & Rollouts (nahtlos mit Task 9)

### Pipeline
1. **Lint/Unit** ‚Üí Code Quality Gates
2. **Image Build+Scan** ‚Üí Security Validation
3. **Deploy to Staging** ‚Üí Integration Testing
4. **Mesh-Canary** (1-5%) ‚Üí Performance Validation
5. **Gates** (QA, Perf, A11y) ‚Üí Quality Assurance
6. **Promote** to 100% ‚Üí Full Deployment
7. **Production** ‚Üí Live Traffic

### Blue/Green mit ECS
- **App Mesh Route Split** (empfohlen) f√ºr Traffic Control
- **ECS Deployments** (minimum healthy 100%, max 200%) als Fallback
- **Health Gates** aus Task 9 f√ºr Deployment Validation
- **Automated Rollback** bei Gate Failures

### Rollback
- **VirtualRouter Weight** ‚Üí sofort auf vorherige VirtualNode
- **Orchestrator** (Task 9) bekommt "container"-Modus
- **Database Migrations** separat mit Forward/Backward Compatibility
- **Feature Flags** f√ºr Business Logic Rollback

## 10) üíæ Daten & Konsistenz

### Database per Service
- **Jeder Service** sein Datenbesitz
- **Keine Cross-DB Queries** zwischen Services
- **Shared Nothing** Architecture
- **Data Access Layer** pro Service

### Konsistenz-Patterns
- **Lesemodelle** via Replikate/Caches
- **Eventual Consistency** akzeptieren und kommunizieren
- **Sagas/Outbox** f√ºr Distributed Transactions
- **Idempotenz** √ºber Request-Keys

### Event Sourcing (Optional)
- **Event Store** f√ºr Audit und Replay
- **CQRS** f√ºr Read/Write Separation
- **Snapshots** f√ºr Performance
- **Event Versioning** f√ºr Schema Evolution

## 11) üß™ Tests & Qualit√§t (von Anfang an, nicht hinten anh√§ngen)

### Test Pyramid
- **Unit** (Service Logik, Retry/Timeout/Circuit Tests)
- **Contract Tests** (Pact / protobuf compatibility)
- **Integration** (LocalStack + docker-compose der Services)
- **E2E** (Minimal, nur Critical User Journeys)

### Resilience/Chaos
- **Fault Injection** (Envoy HTTP faults)
- **App Mesh Outlier Detection** Tests
- **Network Partitions** Simulation
- **Service Unavailability** Testing

### Performance
- **k6/Artillery** gegen Mesh Canary
- **SLO-Checks** (P95<200ms prod)
- **Load Testing** f√ºr Capacity Planning
- **Baseline Establishment** f√ºr Regression Detection

### Security
- **Trivy** Container Scanning
- **IAM Access Analyzer** f√ºr Permissions
- **mTLS-Verifikation** Tests
- **Secret Rotation** Testing

## 12) üîÑ Migration & Betriebsf√ºhrung

### Strangler-Fig Pattern
- **Bestehende Lambda-Endpunkte** schrittweise hinter Ingress ‚Üí Mesh schalten
- **St√ºck f√ºr St√ºck** in ECS √ºberf√ºhren
- **Feature Flags** f√ºr Traffic Routing Control
- **Parallel Running** w√§hrend Migration

### Runbooks
- **Incident Response**: Mesh Route Rollback, Service Restart
- **Throttling/Backoff** erh√∂hen bei Overload
- **DLQ Drain** Procedures
- **Hotfix Deploy** via weight=0‚Üí5% Testing

### Quotas & Limits
- **ECS Tasks/Subnet** ENI Limits beachten
- **App Mesh** Resource Limits (1000 Virtual Nodes)
- **ECR Rate Limits** f√ºr Image Pulls
- **Cloud Map** Service Limits (1000 Services)

## üìã Mini-DoD f√ºr Task 12 (Definition of Done)

### Infrastructure (CDK Stacks)
- [ ] **VPC** (mit Endpoints) f√ºr Cost-Optimization
- [ ] **ECR** mit Multi-Region Replication
- [ ] **ECS/Fargate Cluster** mit Container Insights
- [ ] **Cloud Map** f√ºr Service Discovery
- [ ] **App Mesh** (mesh + vs/vn/vr) mit mTLS
- [ ] **ALB Ingress** mit Path-based Routing

### Services
- [ ] **2 Referenzservices** (auth, persona) als ECS Services
- [ ] **Envoy Sidecar** mit mTLS aktiv
- [ ] **Health Checks** f√ºr ECS und ALB
- [ ] **Structured Logging** mit Correlation IDs

### App Mesh Policies
- [ ] **Timeouts** (2-5s) konfiguriert
- [ ] **Retries** (3x mit Backoff) implementiert
- [ ] **OutlierDetection** (5xx ejection) aktiv
- [ ] **Canary 1-5%** Traffic Split funktional

### CI/CD
- [ ] **Build+Scan** ‚Üí Staging ‚Üí Canary ‚Üí Gates ‚Üí Promote
- [ ] **Rollback** √ºber Route Weights funktional
- [ ] **Health Gates** aus Task 9 integriert
- [ ] **Security Scanning** mit Trivy aktiv

### Observability
- [ ] **Logs+Metrics+Tracing** (ADOT/X-Ray) funktional
- [ ] **Dashboards** f√ºr Service Mesh Visibility
- [ ] **Alarme** f√ºr P95 Latency und Error Rate
- [ ] **Correlation IDs** End-to-End verfolgbar

### Tests
- [ ] **Unit Tests** f√ºr Service Logic
- [ ] **Contract Tests** f√ºr API Compatibility
- [ ] **Integration Tests** mit LocalStack
- [ ] **Canary Performance** Smoke Tests

### Cost & Security
- [ ] **NAT-Traffic niedrig** (VPCEs aktiv)
- [ ] **Tasks right-sized** mit Envoy Overhead
- [ ] **Budget-Alarme** aktiv und getestet
- [ ] **IAM Least Privilege** validiert

## üöÄ Startbefehle (ohne R√ºckfragen, direkt ausf√ºhrbar)

### Infrastructure (CDK)
```bash
npm run cdk:bootstrap && npm run cdk:deploy microservices-foundation --all --profile matbakh-dev
```

### Images
```bash
npm run svc:build && npm run svc:scan && npm run svc:push
```

### Deploy Services
```bash
npm run deploy:svc -- --env staging --mesh-canary 5
```

### Promote/Rollback
```bash
# Promote to full traffic
npm run mesh:promote -- --service persona --to 100

# Emergency rollback
npm run mesh:rollback -- --service persona
```

### Health Check
```bash
# Service health
npm run health:check --service persona --region eu-central-1

# Mesh status
npm run mesh:status --all-services
```

### Monitoring
```bash
# Real-time metrics
npm run metrics:watch --service persona

# Performance analysis
npm run perf:analyze --service persona --duration 1h
```

## üéØ Erfolgs-Kriterien

Wenn diese Punkte beachtet werden, erhalten wir eine:

- **Robuste** Microservices-Basis mit Circuit Breaking und Retry Logic
- **Sichere** Architektur mit mTLS und Least-Privilege IAM
- **Beobachtbare** Services mit End-to-End Tracing und Metrics
- **Kostenbewusste** Implementation mit VPC Endpoints und Right-Sizing
- **Saubere Integration** mit bestehenden Task 9-11 Bausteinen
- **10x-Ambition** Support f√ºr Latenz, Zuverl√§ssigkeit, Developer-Erlebnis

Diese Foundation unterst√ºtzt die Skalierung auf hunderte von Services bei gleichbleibender Operational Excellence und Cost Efficiency.

---

**Implementation Guide Status**: ‚úÖ **READY FOR EXECUTION**  
**Dependencies**: Task 9 ‚úÖ, Task 10 ‚úÖ, Task 11 ‚úÖ  
**Next Step**: Begin with Infrastructure Foundation Setup (Task 12.1)  